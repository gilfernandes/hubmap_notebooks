{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U git+https://github.com/qubvel/segmentation_models.pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install rasterio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys, os, random, time, json\n",
    "import numba, cv2, gc\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as D\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "\n",
    "from segmentation_models_pytorch import FPN\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed = 42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "set_seeds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup training variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /home/jupyter/data/\n",
    "DATA_PATH = Path('/home/jupyter/data_2/')\n",
    "assert DATA_PATH.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPORT_PATH = Path('/home/jupyter/reports')\n",
    "if not REPORT_PATH.exists():\n",
    "    os.makedirs(REPORT_PATH)\n",
    "assert REPORT_PATH.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37_pytorch_fpn_efficientnet_b7_1536_768_shifted_slices_groupkfold_smooth_fold_0\n",
      "37_pytorch_fpn_efficientnet_b7_1536_768_shifted_slices_groupkfold_smooth_fold_1\n",
      "37_pytorch_fpn_efficientnet_b7_1536_768_shifted_slices_groupkfold_smooth_fold_2\n",
      "37_pytorch_fpn_efficientnet_b7_1536_768_shifted_slices_groupkfold_smooth_fold_3\n"
     ]
    }
   ],
   "source": [
    "!ls {REPORT_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = '37_pytorch_fpn_efficientnet_b7_1536_768_shifted_slices_groupkfold_smooth_new_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_decode(mask_rle, shape=(256, 256)):\n",
    "    '''\n",
    "    mask_rle: run-length as string formated (start length)\n",
    "    shape: (height,width) of array to return \n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "\n",
    "    '''\n",
    "    splits = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (splits[0:][::2], splits[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0] * shape[1], dtype='uint8')\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo: hi] = 1\n",
    "    return img.reshape(shape, order='F') # Fortran order reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit()\n",
    "def rle_numba(pixels):\n",
    "    size = len(pixels)\n",
    "    points = []\n",
    "    if pixels[0] == 1: points.append(1)\n",
    "    for i in range(1, size):\n",
    "        if pixels[i] != pixels[i-1]:\n",
    "            if len(points) % 2 == 0:\n",
    "                points.append(i+1)\n",
    "            else:\n",
    "                points.append(i+1 - points[-1])\n",
    "    if pixels[-1] == 1: points.append(size-points[-1]+1)    \n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check run length encoding starting with 0\n",
    "assert rle_numba([0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1]) == [2, 2, 5, 1, 7, 4, 12, 1]\n",
    "# Check run length encoding starting with 0\n",
    "assert rle_numba([1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1]) == [1, 3, 5, 1, 7, 4, 12, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_numba_encode(image):\n",
    "    pixels = image.flatten(order = 'F')\n",
    "    points = rle_numba(pixels)\n",
    "    return ' '.join(str(x) for x in points)\n",
    "\n",
    "def make_grid(shape, window=256, min_overlap=32):\n",
    "    \"\"\"\n",
    "        Return Array of size (N,4), where N - number of tiles,\n",
    "        2nd axis represente slices: x1,x2,y1,y2 \n",
    "    \"\"\"\n",
    "    x, y = shape\n",
    "    nx = x // (window - min_overlap) + 1\n",
    "    x1 = np.linspace(0, x, num=nx, endpoint=False, dtype=np.int64)\n",
    "    x1[-1] = x - window\n",
    "    x2 = (x1 + window).clip(0, x)\n",
    "    ny = y // (window - min_overlap) + 1\n",
    "    y1 = np.linspace(0, y, num=ny, endpoint=False, dtype=np.int64)\n",
    "    y1[-1] = y - window\n",
    "    y2 = (y1 + window).clip(0, y)\n",
    "    slices = np.zeros((nx,ny, 4), dtype=np.int64)\n",
    "    \n",
    "    for i in range(nx):\n",
    "        for j in range(ny):\n",
    "            slices[i,j] = x1[i], x2[i], y1[j], y2[j]    \n",
    "    return slices.reshape(nx*ny,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encoding</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2f6ecfcdf</th>\n",
       "      <td>296084587 4 296115835 6 296115859 14 296147109...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8242609fa</th>\n",
       "      <td>96909968 56 96941265 60 96972563 64 97003861 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaa6a05cc</th>\n",
       "      <td>30989109 59 31007591 64 31026074 68 31044556 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cb2d976f4</th>\n",
       "      <td>78144363 5 78179297 15 78214231 25 78249165 35...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b9a3865fc</th>\n",
       "      <td>61271840 4 61303134 13 61334428 22 61365722 30...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b2dc8411c</th>\n",
       "      <td>56157731 21 56172571 45 56187411 51 56202252 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0486052bb</th>\n",
       "      <td>101676003 6 101701785 8 101727568 9 101753351 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e79de561c</th>\n",
       "      <td>7334642 14 7350821 41 7367001 67 7383180 82 73...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>095bf7a1f</th>\n",
       "      <td>113277795 21 113315936 53 113354083 87 1133922...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54f2eec69</th>\n",
       "      <td>124967057 36 124997425 109 125027828 147 12505...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4ef6695ce</th>\n",
       "      <td>137041956 58 137081912 65 137121869 72 1371618...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26dc41664</th>\n",
       "      <td>245832956 28 245869925 2 245871115 33 24590808...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c68fe75ea</th>\n",
       "      <td>21256809 3 21283644 10 21310479 17 21337315 22...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>afa5e8098</th>\n",
       "      <td>65837968 7 65874765 11 65874827 12 65911562 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1e2425f28</th>\n",
       "      <td>49453112 7 49479881 22 49506657 31 49533433 40...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    encoding\n",
       "id                                                          \n",
       "2f6ecfcdf  296084587 4 296115835 6 296115859 14 296147109...\n",
       "8242609fa  96909968 56 96941265 60 96972563 64 97003861 6...\n",
       "aaa6a05cc  30989109 59 31007591 64 31026074 68 31044556 7...\n",
       "cb2d976f4  78144363 5 78179297 15 78214231 25 78249165 35...\n",
       "b9a3865fc  61271840 4 61303134 13 61334428 22 61365722 30...\n",
       "b2dc8411c  56157731 21 56172571 45 56187411 51 56202252 5...\n",
       "0486052bb  101676003 6 101701785 8 101727568 9 101753351 ...\n",
       "e79de561c  7334642 14 7350821 41 7367001 67 7383180 82 73...\n",
       "095bf7a1f  113277795 21 113315936 53 113354083 87 1133922...\n",
       "54f2eec69  124967057 36 124997425 109 125027828 147 12505...\n",
       "4ef6695ce  137041956 58 137081912 65 137121869 72 1371618...\n",
       "26dc41664  245832956 28 245869925 2 245871115 33 24590808...\n",
       "c68fe75ea  21256809 3 21283644 10 21310479 17 21337315 22...\n",
       "afa5e8098  65837968 7 65874765 11 65874827 12 65911562 15...\n",
       "1e2425f28  49453112 7 49479881 22 49506657 31 49533433 40..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(DATA_PATH / 'train.csv', index_col=[0])\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/jupyter/ds_cache’: File exists\n"
     ]
    }
   ],
   "source": [
    "MASK_PATH = Path('/home/jupyter/ds_cache')\n",
    "!mkdir {MASK_PATH}\n",
    "\n",
    "import shutil\n",
    "\n",
    "def reset_mask_path():\n",
    "    shutil.rmtree(MASK_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW = 1536 # tile size\n",
    "MIN_OVERLAP = 32\n",
    "NEW_SIZE = 768 # size after re-size which are fed to the model\n",
    "THRESHOLD = 0\n",
    "CONTENT_THRESHOLD = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Used to filter tiles with enough color information in it\n",
    "def is_tile_contains_info(img, pixel_limits = (50, 220), content_threshold = CONTENT_THRESHOLD, expected_shape = (WINDOW, WINDOW, 3)):\n",
    "    \"\"\"\n",
    "    img: np.array\n",
    "    pixel_limits: tuple\n",
    "    content_threshold: float percents\n",
    "    expected_shape: tuple\n",
    "    \"\"\"\n",
    "    \n",
    "    left_limit = np.prod(img > pixel_limits[0], axis=-1)\n",
    "    right_limit =  np.prod(img < pixel_limits[1], axis=-1)\n",
    "\n",
    "    if img.shape != expected_shape:\n",
    "        print('img.shape != expected_shape', img.shape)\n",
    "        return False, 0.\n",
    "\n",
    "    percent_of_pixels = np.sum(left_limit*right_limit) / (img.shape[0] * img.shape[1])\n",
    "    return  percent_of_pixels > content_threshold, percent_of_pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identity = rasterio.Affine(1, 0, 0, 0, 1, 0)\n",
    "identity = None\n",
    "# normalize_transform = T.Normalize([0.625, 0.448, 0.688], [0.131, 0.177, 0.101])\n",
    "# normalize_transform = T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "# normalize_transform = T.Normalize([0.65459856,0.48386562,0.69428385], [0.15167958,0.23584107,0.13146145])\n",
    "normalize_transform = T.Normalize([0.6130, 0.4126, 0.6595], [0.1417, 0.2045, 0.1237])\n",
    "\n",
    "def read_from_slice(dataset, layers, x1, x2, y1, y2):\n",
    "    if dataset.count == 3:\n",
    "        image = dataset.read([1,2,3],\n",
    "                    window=Window.from_slices((x1,x2),(y1,y2)))\n",
    "        image = np.moveaxis(image, 0, -1)\n",
    "    else:\n",
    "        image = np.zeros((WINDOW, WINDOW, 3), dtype=np.uint8)\n",
    "        for fl in range(3):\n",
    "            image[:,:,fl] = layers[fl].read(window=Window.from_slices((x1,x2),(y1,y2)))\n",
    "    return image.astype(np.uint8)\n",
    "\n",
    "def extract_layers(dataset, filepath):\n",
    "    layers = None\n",
    "    if dataset.count != 3:\n",
    "        print(f'Image file ({filepath}) with subdatasets as channels')\n",
    "        layers = [rasterio.open(subd) for subd in dataset.subdatasets]\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HubSlices():\n",
    "    \n",
    "    def __init__(self, root_dir, window=256, overlap=32, threshold=THRESHOLD):\n",
    "        self.path = root_dir\n",
    "        assert self.path.exists()\n",
    "        self.csv = pd.read_csv(self.path / 'train.csv', index_col=[0])\n",
    "        self.window = window\n",
    "        self.overlap = overlap\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def build_slices(self):\n",
    "        self.masks = []; self.files = []; self.slices = []\n",
    "        self.skipped = 0\n",
    "        slices_path = MASK_PATH/f'slices.pkl'\n",
    "        files_path = MASK_PATH/f'files.pkl'\n",
    "        for i, filename in tqdm(enumerate(self.csv.index), total = len(self.csv)):\n",
    "            filepath = self.path/'train'/f'{filename}.tiff'\n",
    "            assert filepath.exists()\n",
    "            self.files.append(filepath)\n",
    "            with rasterio.open(filepath) as dataset:\n",
    "                self.build_slice(dataset, filename, i)\n",
    "            print(f'Finished {filename}')\n",
    "        with open(slices_path, \"wb\") as filehandler:\n",
    "            pickle.dump(self.slices, filehandler)\n",
    "        with open(files_path, \"wb\") as filehandler:\n",
    "            pickle.dump(self.files, filehandler)\n",
    "        for i, mask in enumerate(self.masks):\n",
    "            with open(MASK_PATH/f'masks_{i}.pkl', \"wb\") as filehandler:\n",
    "                pickle.dump(mask, filehandler)\n",
    "            \n",
    "    def build_slice(self, dataset, filename, i):\n",
    "        dataset_shape = dataset.shape\n",
    "        self.masks.append(rle_decode(self.csv.loc[filename, 'encoding'], dataset_shape))\n",
    "        slices = make_grid(dataset_shape, window = self.window, min_overlap = self.overlap)\n",
    "\n",
    "        # Shifting slices to the right and bottom and adding to the original slices\n",
    "        slices_copy = slices.copy()\n",
    "        slices_copy_y = slices.copy()\n",
    "#         # horizontal\n",
    "        slices_copy[:,(0,1)] += WINDOW // 2 # shift\n",
    "        slices = np.concatenate ([slices, slices_copy])\n",
    "#         # vertical\n",
    "        slices_copy_y[:,(2,3)] += WINDOW // 2\n",
    "        slices = np.concatenate ([slices, slices_copy_y])\n",
    "        slices = slices[~(slices[:,1] > dataset_shape[0]),:] # filter those outside of the screen\n",
    "        slices = slices[~(slices[:,3] > dataset_shape[1]),:] # filter those outside of the screen\n",
    "        \n",
    "        layers = extract_layers(dataset, filename)\n",
    "        \n",
    "        # Only including slices above a specific threshold\n",
    "        # Note: we are potentially throwing away some data here\n",
    "        for slc in slices:\n",
    "            x1, x2, y1, y2 = slc\n",
    "            image = read_from_slice(dataset, layers, x1, x2 , y1, y2)\n",
    "#             contains_info = is_tile_contains_info(image)\n",
    "#             if self.masks[-1][x1:x2,y1:y2].sum() > self.threshold and contains_info[0]:\n",
    "            if self.masks[-1][x1:x2,y1:y2].sum() > self.threshold:\n",
    "                self.slices.append([i,x1,x2,y1,y2])\n",
    "            else:\n",
    "                self.skipped += 1\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"slices: {len(self.slices)} skipped: {self.skipped}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_slices = HubSlices(DATA_PATH, window=WINDOW, overlap=MIN_OVERLAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]/opt/conda/lib/python3.6/site-packages/rasterio/__init__.py:207: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix be returned.\n",
      "  s = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "  7%|▋         | 1/15 [01:31<21:18, 91.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 2f6ecfcdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 2/15 [04:14<28:59, 133.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 8242609fa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 3/15 [04:25<15:31, 77.65s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished aaa6a05cc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 4/15 [07:41<22:49, 124.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished cb2d976f4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 5/15 [10:11<22:16, 133.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished b9a3865fc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 6/15 [10:31<14:13, 94.81s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished b2dc8411c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 7/15 [12:12<12:54, 96.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 0486052bb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 8/15 [12:18<07:55, 67.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished e79de561c\n",
      "Image file (095bf7a1f) with subdatasets as channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 9/15 [13:43<07:20, 73.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 095bf7a1f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 10/15 [14:04<04:45, 57.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 54f2eec69\n",
      "Image file (4ef6695ce) with subdatasets as channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 11/15 [16:09<05:11, 77.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 4ef6695ce\n",
      "Image file (26dc41664) with subdatasets as channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 12/15 [17:38<04:03, 81.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 26dc41664\n",
      "Image file (c68fe75ea) with subdatasets as channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 13/15 [19:09<02:48, 84.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished c68fe75ea\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 14/15 [19:34<01:06, 66.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished afa5e8098\n",
      "Image file (1e2425f28) with subdatasets as channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [20:29<00:00, 81.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 1e2425f28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "hub_slices.build_slices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HubDataset(D.Dataset):\n",
    "    def __init__(self, root_dir, transform, valid_transform=None, mode='train', window=WINDOW, overlap=MIN_OVERLAP, threshold = THRESHOLD):\n",
    "        self.path = root_dir\n",
    "        assert self.path.exists()\n",
    "        self.overlap, self.window, self.transform, self.valid_transform, self.threshold = overlap, window, transform, valid_transform, threshold\n",
    "        self.mode = mode\n",
    "        self.csv = pd.read_csv(self.path / 'train.csv', index_col=[0])\n",
    "        self.build_slices()\n",
    "        self.len = len(self.slices)\n",
    "        # where do these numbers come from?\n",
    "        # Better to calculate them to check if correct.\n",
    "        self.as_tensor = T.Compose([\n",
    "            T.ToTensor(),\n",
    "            normalize_transform,\n",
    "        ])\n",
    "        \n",
    "    def __copy__(self):\n",
    "        new_ds = type(self)(\n",
    "            self.path,\n",
    "            self.transform,\n",
    "            valid_transform=self.valid_transform,\n",
    "            mode=self.mode,\n",
    "            window=self.window,\n",
    "            overlap=self.overlap,\n",
    "            threshold=self.threshold\n",
    "        )\n",
    "        new_ds.masks = self.masks\n",
    "        new_ds.files = self.files\n",
    "        new_ds.slices = self.slices\n",
    "        new_ds.skipped = self.skipped\n",
    "        return new_ds\n",
    "        \n",
    "    def build_slices(self):\n",
    "        self.masks = []; self.files = []; self.slices = []\n",
    "        self.skipped = 0\n",
    "        slices_path = MASK_PATH/f'slices.pkl'\n",
    "        files_path = MASK_PATH/f'files.pkl'\n",
    "        masks_path = MASK_PATH/f'masks.pkl'\n",
    "        if not slices_path.exists():\n",
    "            for i, filename in tqdm(enumerate(self.csv.index), total = len(self.csv)):\n",
    "                filepath = self.path/'train'/f'{filename}.tiff'\n",
    "                assert filepath.exists()\n",
    "                self.files.append(filepath)\n",
    "                with rasterio.open(filepath) as dataset:\n",
    "                    self.build_slice(dataset, filename, i)\n",
    "                print(f'Finished {filename}')\n",
    "            with open(slices_path, \"wb\") as filehandler:\n",
    "                pickle.dump(self.slices, filehandler)\n",
    "            with open(files_path, \"wb\") as filehandler:\n",
    "                pickle.dump(self.files, filehandler)\n",
    "            for i, mask in enumerate(self.masks):\n",
    "                with open(MASK_PATH/f'masks_{i}.pkl', \"wb\") as filehandler:\n",
    "                    pickle.dump(mask, filehandler)\n",
    "        else:\n",
    "            print('Reading cached slices, files and masks')\n",
    "            with open(slices_path,'rb') as file:\n",
    "                self.slices = pickle.load(file)\n",
    "            with open(files_path,'rb') as file:\n",
    "                self.files = pickle.load(file)\n",
    "            list(MASK_PATH.glob(\"mask_*\"))\n",
    "            for mask_path in :\n",
    "                with open(masks_path,'rb') as file:\n",
    "                    self.masks.append[pickle.load(file)]\n",
    "                \n",
    "    def build_slice(self, dataset, filename, i):\n",
    "        dataset_shape = dataset.shape\n",
    "        self.masks.append(rle_decode(self.csv.loc[filename, 'encoding'], dataset_shape))\n",
    "        slices = make_grid(dataset_shape, window = self.window, min_overlap = self.overlap)\n",
    "\n",
    "        # Shifting slices to the right and bottom and adding to the original slices\n",
    "        slices_copy = slices.copy()\n",
    "        slices_copy_y = slices.copy()\n",
    "#         # horizontal\n",
    "        slices_copy[:,(0,1)] += WINDOW // 2 # shift\n",
    "        slices = np.concatenate ([slices, slices_copy])\n",
    "#         # vertical\n",
    "        slices_copy_y[:,(2,3)] += WINDOW // 2\n",
    "        slices = np.concatenate ([slices, slices_copy_y])\n",
    "        slices = slices[~(slices[:,1] > dataset_shape[0]),:] # filter those outside of the screen\n",
    "        slices = slices[~(slices[:,3] > dataset_shape[1]),:] # filter those outside of the screen\n",
    "        \n",
    "        layers = extract_layers(dataset, filename)\n",
    "        \n",
    "        # Only including slices above a specific threshold\n",
    "        # Note: we are potentially throwing away some data here\n",
    "        for slc in slices:\n",
    "            x1, x2, y1, y2 = slc\n",
    "            image = read_from_slice(dataset, layers, x1, x2 , y1, y2)\n",
    "#             contains_info = is_tile_contains_info(image)\n",
    "#             if self.masks[-1][x1:x2,y1:y2].sum() > self.threshold and contains_info[0]:\n",
    "            if self.masks[-1][x1:x2,y1:y2].sum() > self.threshold:\n",
    "                self.slices.append([i,x1,x2,y1,y2])\n",
    "            else:\n",
    "                self.skipped += 1\n",
    "                        \n",
    "                        \n",
    "    def apply_transform(self, image, mask):\n",
    "        augments = self.transform(image=image, mask=mask) if self.mode == 'train' else self.valid_transform(image=image, mask=mask)\n",
    "        image = self.as_tensor(augments['image'])\n",
    "        mask = augments['mask'][None]\n",
    "        mask_torch = torch.from_numpy(mask).to(torch.float16)\n",
    "        return image, mask_torch\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        image_path = MASK_PATH/f'image_{index}'\n",
    "        mask_path = MASK_PATH/f'mask_{index}'\n",
    "        if not image_path.exists():\n",
    "            idx = self.slices[index][0]\n",
    "            filename = self.files[idx]\n",
    "            x1, x2, y1, y2 = self.slices[index][1:]\n",
    "            with rasterio.open(filename) as dataset:\n",
    "                layers = extract_layers(dataset, filename)\n",
    "                image = read_from_slice(dataset, x1, x2, y1, y2).astype('uint8')\n",
    "            mask = self.masks[idx][x1:x2,y1:y2]\n",
    "            with open(image_path, \"wb\") as filehandler:\n",
    "                pickle.dump(image, filehandler)\n",
    "                if index % 100 == 0:\n",
    "                    print(f'Writing to {image_path}')\n",
    "            with open(mask_path, \"wb\") as filehandler:\n",
    "                pickle.dump(mask, filehandler)\n",
    "            return self.apply_transform(image, mask)\n",
    "        else:\n",
    "            with open(image_path,'rb') as file:\n",
    "                image = pickle.load(file)\n",
    "            with open(mask_path,'rb') as file:\n",
    "                mask = pickle.load(file)\n",
    "            return self.apply_transform(image, mask)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'total: {len(self)}, skipped: {self.skipped} mode: {self.mode}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_mask_path()\n",
    "!mkdir {MASK_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ds(size):\n",
    "    trfm = A.Compose([\n",
    "        A.Resize(size, size, p=1.0),\n",
    "        A.HorizontalFlip(),\n",
    "        A.VerticalFlip(),\n",
    "        A.RandomRotate90(),\n",
    "        A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=20, p=0.9, \n",
    "                         border_mode=cv2.BORDER_REFLECT),\n",
    "        A.OneOf([\n",
    "            A.OpticalDistortion(p=0.4),\n",
    "            A.GridDistortion(p=.1),\n",
    "            A.IAAPiecewiseAffine(p=0.4),\n",
    "        ], p=0.3),\n",
    "        A.OneOf([\n",
    "            A.HueSaturationValue(10,15,10),\n",
    "            A.CLAHE(clip_limit=3),\n",
    "            A.RandomBrightnessContrast(),            \n",
    "        ], p=0.5)\n",
    "    ], p=1.0)\n",
    "    \n",
    "    valid_transform = A.Compose([\n",
    "        A.Resize(size, size, p=1.0),\n",
    "        A.HorizontalFlip(),\n",
    "        A.VerticalFlip(),\n",
    "        A.RandomRotate90()\n",
    "    ])\n",
    "\n",
    "    return HubDataset(DATA_PATH, window=WINDOW, overlap=MIN_OVERLAP, transform=trfm, valid_transform=valid_transform)\n",
    "\n",
    "ds = generate_ds(NEW_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with rasterio.open('/home/jupyter/data/train/2f6ecfcdf.tiff') as raster:\n",
    "#     img = raster.read([1,2,3], window=Window.from_slices((3909, 4933),(11464,12488)))\n",
    "#     img = np.moveaxis(img, 0, -1)\n",
    "#     print(img.shape)\n",
    "#     crs = raster.crs\n",
    "\n",
    "# plt.figure(figsize = (20,20))\n",
    "# plt.imshow(img)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slices = make_grid(raster.shape, window = WINDOW, min_overlap = MIN_OVERLAP)\n",
    "# slices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slices_copy_y = slices.copy()\n",
    "# slices_copy_y[:,(2,3)] += WINDOW // 2\n",
    "# slices_copy_y = slices_copy_y[~(slices_copy[:,3] > raster.shape[1]),:]\n",
    "# slices = np.concatenate ([slices, slices_copy_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_mask_img(idx):\n",
    "    image, mask = ds[idx]\n",
    "    mask = mask.to(torch.uint8)\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(mask[0], cmap='gray')\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(np.moveaxis(image.numpy(), 0, -1));\n",
    "\n",
    "for i in range(5):\n",
    "    display_mask_img(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.mode = 'valid'\n",
    "\n",
    "for i in range(5):\n",
    "    display_mask_img(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.mode = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, mask = ds[0]\n",
    "\n",
    "_ = rle_numba_encode(mask[0].numpy().astype('uint8')) # compile function with numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images and its corresponding masks are saved with the same filename.\n",
    "groups = [ds.slices[i][0] for i in range(len(ds))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_kfold = GroupKFold(n_splits = FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_split_on_index(img_index = 7):\n",
    "#     valid_idx, train_idx = [], []\n",
    "#     for i in range(len(ds)):\n",
    "#         if ds.slices[i][0] == img_index:\n",
    "#             valid_idx.append(i)\n",
    "#         else:\n",
    "#             train_idx.append(i)\n",
    "#     return valid_idx, train_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 6\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def create_subset(ds, train_idx, valid_idx):\n",
    "    train_ds = D.Subset(ds, train_idx)\n",
    "    val_ds = copy.copy(ds)\n",
    "    val_ds.mode = 'valid'\n",
    "    valid_ds = D.Subset(val_ds, valid_idx)\n",
    "    print(val_ds)\n",
    "    return train_ds, valid_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_valid_dls(ds, train_idx, valid_idx):\n",
    "    train_ds, valid_ds = create_subset(ds, train_idx, valid_idx)\n",
    "\n",
    "    num_workers = 0\n",
    "    # define training and validation data loaders\n",
    "    train_dl = D.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "    valid_dl = D.DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers)\n",
    "    return train_dl, valid_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER_NAME = 'efficientnet-b7'\n",
    "\n",
    "class HuBMAPModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HuBMAPModel, self).__init__()\n",
    "        self.model = FPN(encoder_name = ENCODER_NAME, \n",
    "                          encoder_weights = 'imagenet',\n",
    "                          classes = 1,\n",
    "                          activation = None,\n",
    "                          aux_params = None)\n",
    "    def forward(self, images):\n",
    "        return self.model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    return HuBMAPModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "WD = 1e-3\n",
    "LABEL_SMOOTH = 0.01\n",
    "GRAD_ACCU_STEPS = 1\n",
    "BEST_MODEL = f'best_model_fpn_efficientnetb7_1024_768_double_shift_{ENCODER_NAME}.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer_scheduler(model, train_dl, epochs):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=LR,\n",
    "                                                    steps_per_epoch=len(train_dl), epochs=epochs)\n",
    "    return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftDiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1., dims=(-2,-1)):\n",
    "\n",
    "        super(SoftDiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "        self.dims = dims\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "\n",
    "        tp = (x * y).sum(self.dims)\n",
    "        fp = (x * (1 - y)).sum(self.dims)\n",
    "        fn = ((1 - x) * y).sum(self.dims)\n",
    "        \n",
    "        dc = (2 * tp + self.smooth) / (2 * tp + fp + fn + self.smooth)\n",
    "        dc = dc.mean()\n",
    "\n",
    "        return 1 - dc\n",
    "    \n",
    "bce_fn = nn.BCEWithLogitsLoss()\n",
    "dice_fn = SoftDiceLoss()\n",
    "\n",
    "def loss_fn(y_pred, y_true):\n",
    "    bce = bce_fn(y_pred, y_true)\n",
    "    dice = dice_fn(y_pred.sigmoid(), y_true)\n",
    "    return 0.5 * bce + 0.5 * dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_loss = smp.losses.DiceLoss(mode='binary', from_logits=False)\n",
    "\n",
    "def loss_fn(y_pred, y_true):\n",
    "    return dice_loss(y_pred.sigmoid(), y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSS_ALPHA = 0.4\n",
    "\n",
    "def loss_fn(y_pred, y_true):\n",
    "    return LOSS_ALPHA * dice_loss(y_pred.sigmoid(), y_true) + (1 - LOSS_ALPHA) * bce_fn(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jaccard_loss = smp.utils.losses.JaccardLoss()\n",
    "\n",
    "# def loss_fn(y_pred, y_true):\n",
    "#     return jaccard_loss(y_pred.sigmoid(), y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_metric(y_pred, y, epsilon = 1e-7):\n",
    "    dims=(-2,-1)\n",
    "    x = (y_pred > 0).float()\n",
    "    dc = (2 * (x * y).sum(dims) + epsilon) / ((x + y).sum(dims) + epsilon)\n",
    "    return dc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_metric = smp.utils.metrics.IoU(threshold=0.5)\n",
    "dice_metric_2 = smp.utils.metrics.Fscore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "    \n",
    "# def smooth_mask(mask, alpha = 0.9):\n",
    "#     zeros_mask = (mask == 0).to(torch.float16) * (1 - alpha)\n",
    "#     mask = mask * alpha\n",
    "#     return mask + zeros_mask\n",
    "\n",
    "def smooth_mask_2(mask, alpha = LABEL_SMOOTH):\n",
    "    return (1 - alpha) * mask + alpha / 2\n",
    "\n",
    "def train_epoch(model, dataloader, optim, criterion, scheduler, device=\"cpu\", grad_accu_steps=GRAD_ACCU_STEPS):\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = []\n",
    "    labels = []\n",
    "    outs = []\n",
    "    lrs = []\n",
    "    \n",
    "    tbar = tqdm(dataloader, position=0, leave=True)\n",
    "    scaler = torch.cuda.amp.GradScaler() # mixed precision support\n",
    "    scale = None\n",
    "    for step, (image, target) in enumerate(tbar):\n",
    "        image, target = image.to(DEVICE), target.float().to(DEVICE)\n",
    "        target = smooth_mask_2(target)\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "            output = model(image)\n",
    "            loss = criterion(output, target) / grad_accu_steps\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        if (step + 1) % grad_accu_steps == 0:\n",
    "            scaler.step(optim)\n",
    "            scale = scaler.get_scale()\n",
    "            scaler.update()\n",
    "            optim.zero_grad()\n",
    "        \n",
    "        skip_lr_sched = (scale != scaler.get_scale())\n",
    "        if not skip_lr_sched:\n",
    "            scheduler.step()\n",
    "        \n",
    "        loss_val = loss.item() * grad_accu_steps\n",
    "        train_loss.append(loss_val)\n",
    "        lrs.append(get_lr(optim))\n",
    "        \n",
    "        tbar.set_description('loss - {:.4f}'.format(loss_val))\n",
    "        \n",
    "    print(f'Train loss: {np.array(train_loss).mean()}')\n",
    "    return train_loss, lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = [dice_metric, dice_metric_2, iou_metric]\n",
    "\n",
    "def val_epoch(model, dataloader, criterion, epoch, device=\"cpu\"):\n",
    "    model.eval()\n",
    "\n",
    "    valid_loss = []\n",
    "    num_corrects = 0\n",
    "    num_total = 0\n",
    "    labels = []\n",
    "    outs = []\n",
    "    metric_list = [[] for _ in all_metrics]\n",
    "\n",
    "    for item in dataloader:\n",
    "        image, target = item\n",
    "        image, target = image.to(DEVICE), target.float().to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(image)\n",
    "            loss = criterion(output, target)\n",
    "            for ml, m in zip(metric_list, all_metrics):\n",
    "                ml.append(m(output, target).item())\n",
    "        valid_loss.append(loss.item())\n",
    "\n",
    "    avg_loss = np.array(valid_loss).mean()\n",
    "    print(f'Epoch {epoch} - valid loss: {avg_loss}')\n",
    "    dice_metric_mean = np.array(metric_list[0]).mean()\n",
    "    dice_metric_mean_2 = np.array(metric_list[1]).mean()\n",
    "    iou_metric_mean = np.array(metric_list[2]).mean()\n",
    "    return valid_loss, dice_metric_mean, avg_loss, iou_metric_mean, dice_metric_mean_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(fold, epochs, train_dl, valid_dl, optimizer, scheduler, patience = 6):\n",
    "    best_loss = 100.0\n",
    "    best_metric = 0\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    accumulated_lrs = []\n",
    "    accumulated_dice_metrics = []\n",
    "    early_stop_counter = 0\n",
    "    messages = []\n",
    "\n",
    "    for epoch in tqdm(range(epochs), position=0, leave=True):\n",
    "        train_loss, lrs = train_epoch(model, train_dl, optimizer, loss_fn, scheduler, DEVICE)\n",
    "        valid_loss, dice_metric_mean, avg_loss, iou_metric_mean, dice_metric_mean_2 = val_epoch(model, valid_dl, loss_fn, epoch, DEVICE)\n",
    "        train_losses += train_loss\n",
    "        valid_losses.append(np.array(valid_loss).mean())\n",
    "        accumulated_lrs += lrs\n",
    "        accumulated_dice_metrics.append(dice_metric_mean)\n",
    "        if best_metric < dice_metric_mean:\n",
    "            best_metric = dice_metric_mean\n",
    "            print('Saving model')\n",
    "            torch.save(model.state_dict(), BEST_MODEL)\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "        if best_loss > avg_loss:\n",
    "            best_loss = avg_loss\n",
    "        print(f'Epoch {epoch} - val best loss {best_loss} dice metric ({dice_metric_mean}, {dice_metric_mean_2}) iou metric ({iou_metric_mean}).')\n",
    "        messages.append({\n",
    "            'fold': fold,\n",
    "            'epoch': epoch,\n",
    "            'best_loss': best_loss,\n",
    "            'dice_metric_mean': dice_metric_mean,\n",
    "            'dice_coeff_mean': dice_metric_mean_2,\n",
    "            'iou_metric_mean': iou_metric_mean\n",
    "        })\n",
    "        with open(REPORT_PATH/f'{EXPERIMENT_NAME}_fold_{fold}', 'w') as outfile:\n",
    "            json.dump(messages, outfile)\n",
    "        if early_stop_counter >= patience:\n",
    "            print('Stopping early')\n",
    "            break\n",
    "    \n",
    "    return train_losses, valid_losses, accumulated_lrs, accumulated_dice_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset_mask_path()\n",
    "# !mkdir {MASK_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 15\n",
    "PATIENCE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_split(fold_info, fold):\n",
    "    global model\n",
    "    \n",
    "    print(f'Processing fold {fold}')\n",
    "    model = get_model()\n",
    "    model.to(DEVICE)\n",
    "    train_idx, valid_idx = fold_info[fold]\n",
    "    f'Proportions valid / train: {len(valid_idx) / len(train_idx)}'\n",
    "    train_dl, valid_dl = generate_train_valid_dls(ds, train_idx, valid_idx)\n",
    "    optimizer, scheduler = create_optimizer_scheduler(model, train_dl, EPOCHS)\n",
    "    train_losses, valid_losses, accumulated_lrs, accumulated_dice_metrics = train(fold, EPOCHS, train_dl, valid_dl, optimizer, scheduler, patience = PATIENCE)\n",
    "    return train_losses, valid_losses, accumulated_lrs, accumulated_dice_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_info = [(train_idx, valid_idx) for fold, (train_idx, valid_idx) in tqdm(enumerate(group_kfold.split(ds.slices, \n",
    "                                                        groups = groups)), total=FOLDS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fastai.data.core import DataLoaders\n",
    "\n",
    "# train_idx, valid_idx = fold_info[0]\n",
    "# train_ds, valid_ds = create_subset(ds, train_idx, valid_idx)\n",
    "\n",
    "# dls = DataLoaders.from_dsets(train_ds, valid_ds, bs=BATCH_SIZE, num_workers=0)\n",
    "# assert(dls.bs == BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = get_model()\n",
    "# metrics = [dice_metric, dice_metric_2, iou_metric]\n",
    "# criterion = loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fastai.vision.all import Learner\n",
    "# from fastai.callback.fp16 import *\n",
    "\n",
    "# learn = Learner(dls, model, loss_func=criterion, lr=LR, metrics=metrics, cbs=[MixedPrecision])\n",
    "# learn = learn.to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# learn.fit_one_cycle(EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_idx, valid_idx = fold_info[0]\n",
    "# train_dl, valid_dl = generate_train_valid_dls(ds, train_idx, valid_idx)\n",
    "# for image, target in tqdm(train_dl):\n",
    "#     pass\n",
    "# #     assert image.shape[0] > 1\n",
    "# for image, target in tqdm(valid_dl):\n",
    "#     pass\n",
    "# #     assert image.shape[0] > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_split_and_move(fold_info, fold):\n",
    "    train_losses, valid_losses, accumulated_lrs, accumulated_dice_metrics = train_split(fold_info, fold)\n",
    "    !mv {BEST_MODEL} {fold}_{BEST_MODEL}\n",
    "    stats_df = pd.DataFrame({'train_losses': train_losses, 'accumulated_lrs': accumulated_lrs})\n",
    "    stats_df[['train_losses']].plot()\n",
    "    val_stats_df = pd.DataFrame({'valid_losses': valid_losses})\n",
    "    val_stats_df[['valid_losses']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_split_and_move(fold_info, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_split_and_move(fold_info, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_split_and_move(fold_info, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_split_and_move(fold_info, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv *.pth models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

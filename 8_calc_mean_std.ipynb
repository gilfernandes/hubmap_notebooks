{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import sys, os, random, time\n",
    "import numba, cv2, gc\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as D\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('/home/jupyter/data/')\n",
    "assert DATA_PATH.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_decode(mask_rle, shape=(256, 256)):\n",
    "    '''\n",
    "    mask_rle: run-length as string formated (start length)\n",
    "    shape: (height,width) of array to return \n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "\n",
    "    '''\n",
    "    splits = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (splits[0:][::2], splits[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0] * shape[1], dtype='uint8')\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo: hi] = 1\n",
    "    return img.reshape(shape, order='F') # Fortran order reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_grid(shape, window=256, min_overlap=32):\n",
    "    \"\"\"\n",
    "        Return Array of size (N,4), where N - number of tiles,\n",
    "        2nd axis represente slices: x1,x2,y1,y2 \n",
    "    \"\"\"\n",
    "    x, y = shape\n",
    "    nx = x // (window - min_overlap) + 1\n",
    "    x1 = np.linspace(0, x, num=nx, endpoint=False, dtype=np.int64)\n",
    "    x1[-1] = x - window\n",
    "    x2 = (x1 + window).clip(0, x)\n",
    "    ny = y // (window - min_overlap) + 1\n",
    "    y1 = np.linspace(0, y, num=ny, endpoint=False, dtype=np.int64)\n",
    "    y1[-1] = y - window\n",
    "    y2 = (y1 + window).clip(0, y)\n",
    "    slices = np.zeros((nx,ny, 4), dtype=np.int64)\n",
    "    \n",
    "    for i in range(nx):\n",
    "        for j in range(ny):\n",
    "            slices[i,j] = x1[i], x2[i], y1[j], y2[j]    \n",
    "    return slices.reshape(nx*ny,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identity = rasterio.Affine(1, 0, 0, 0, 1, 0)\n",
    "identity = None\n",
    "\n",
    "def read_from_slice(dataset, x1, x2, y1, y2):\n",
    "    image = dataset.read([1,2,3],\n",
    "                    window=Window.from_slices((x1,x2),(y1,y2)))\n",
    "    image = np.moveaxis(image, 0, -1)\n",
    "    return image\n",
    "\n",
    "class HubDataset(D.Dataset):\n",
    "    def __init__(self, root_dir, transform, window=256, overlap=32, threshold = 100):\n",
    "        self.path = root_dir\n",
    "        assert self.path.exists()\n",
    "        self.overlap, self.window, self.transform, self.threshold = overlap, window, transform, threshold\n",
    "        self.csv = pd.read_csv(self.path / 'train.csv', index_col=[0])\n",
    "        self.build_slices()\n",
    "        self.len = len(self.slices)\n",
    "        # where do these numbers come from?\n",
    "        # Better to calculate them to check if correct.\n",
    "        self.as_tensor = T.Compose([\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "        \n",
    "        \n",
    "    def build_slices(self):\n",
    "        self.masks = []; self.files = []; self.slices = []\n",
    "        self.skipped = 0\n",
    "        for i, filename in tqdm(enumerate(self.csv.index), total = len(self.csv)):\n",
    "            filepath = self.path/'train'/f'{filename}.tiff'\n",
    "            assert filepath.exists()\n",
    "            self.files.append(filepath)\n",
    "            with rasterio.open(filepath) as dataset:\n",
    "                dataset_shape = dataset.shape\n",
    "                self.masks.append(rle_decode(self.csv.loc[filename, 'encoding'], dataset_shape))\n",
    "                slices = make_grid(dataset_shape, window = self.window, min_overlap = self.overlap)\n",
    "                # Only including slices above a specific threshold\n",
    "                # Note: we are potentially throwing away some data here\n",
    "                for slc in slices:\n",
    "                    x1, x2, y1, y2 = slc\n",
    "                    if self.masks[-1][x1:x2,y1:y2].sum() > self.threshold:\n",
    "                        self.slices.append([i,x1,x2,y1,y2])\n",
    "                    else:\n",
    "                        self.skipped += 1\n",
    "                        \n",
    "                        \n",
    "    def apply_transform(self, image, mask):\n",
    "        augments = self.transform(image=image, mask=mask)\n",
    "        image = self.as_tensor(augments['image'])\n",
    "        mask = augments['mask'][None]\n",
    "        return image, mask\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        image_path = MASK_PATH/f'image_{index}'\n",
    "        mask_path = MASK_PATH/f'mask_{index}'\n",
    "        if not image_path.exists():\n",
    "            idx = self.slices[index][0]\n",
    "            filename = self.files[idx]\n",
    "            x1, x2, y1, y2 = self.slices[index][1:]\n",
    "            with rasterio.open(filename) as dataset:\n",
    "                image = read_from_slice(dataset, x1, x2, y1, y2)\n",
    "            mask = self.masks[idx][x1:x2,y1:y2]\n",
    "            with open(image_path, \"wb\") as filehandler:\n",
    "                pickle.dump(image, filehandler)\n",
    "                if index % 100 == 0:\n",
    "                    print(f'Writing to {image_path}')\n",
    "            with open(mask_path, \"wb\") as filehandler:\n",
    "                pickle.dump(mask, filehandler)\n",
    "            return self.apply_transform(image, mask)\n",
    "        else:\n",
    "            with open(image_path,'rb') as file:\n",
    "                image = pickle.load(file)\n",
    "            with open(mask_path,'rb') as file:\n",
    "                mask = pickle.load(file)\n",
    "            return self.apply_transform(image, mask)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'total: {len(self)}, skipped: {self.skipped}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/jupyter/ds_cache’: File exists\n"
     ]
    }
   ],
   "source": [
    "MASK_PATH = Path('/home/jupyter/ds_cache')\n",
    "!mkdir {MASK_PATH}\n",
    "\n",
    "import shutil\n",
    "\n",
    "def reset_mask_path():\n",
    "    shutil.rmtree(MASK_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW=1024\n",
    "MIN_OVERLAP=32\n",
    "NEW_SIZE=256\n",
    "MINI_SIZE=NEW_SIZE // 2\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1b664905cbf439a8323f384a5168763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate_ds(size):\n",
    "    trfm = A.Compose([\n",
    "        A.Resize(size, size)\n",
    "    ])\n",
    "\n",
    "    return HubDataset(DATA_PATH, window=WINDOW, overlap=MIN_OVERLAP, transform=trfm)\n",
    "\n",
    "ds = generate_ds(NEW_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_std(train_dl):\n",
    "    '''\n",
    "    Calculate the mean and std\n",
    "    var = E[x**2] - E[x]**2\n",
    "    '''\n",
    "    channels_sum, channels_squared_sum, num_batches = 0, 0, 0\n",
    "\n",
    "    for data, _ in tqdm(train_dl, total=len(train_dl)):\n",
    "        channels_sum += torch.mean(data, dim=[0, 2, 3])\n",
    "        channels_squared_sum += torch.mean(data ** 2, dim=[0, 2, 3])\n",
    "        num_batches += 1\n",
    "\n",
    "    assert num_batches == len(train_dl)\n",
    "    mean = channels_sum / num_batches\n",
    "    std = (channels_squared_sum / num_batches - mean ** 2) ** 0.5\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = D.DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9bbd2088be549738641b688763db9a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /home/jupyter/ds_cache/image_100\n",
      "Writing to /home/jupyter/ds_cache/image_200\n",
      "Writing to /home/jupyter/ds_cache/image_300\n",
      "Writing to /home/jupyter/ds_cache/image_400\n",
      "Writing to /home/jupyter/ds_cache/image_500\n",
      "Writing to /home/jupyter/ds_cache/image_600\n",
      "Writing to /home/jupyter/ds_cache/image_700\n",
      "Writing to /home/jupyter/ds_cache/image_800\n",
      "Writing to /home/jupyter/ds_cache/image_900\n",
      "Writing to /home/jupyter/ds_cache/image_1000\n",
      "Writing to /home/jupyter/ds_cache/image_1100\n",
      "Writing to /home/jupyter/ds_cache/image_1200\n",
      "Writing to /home/jupyter/ds_cache/image_1300\n",
      "Writing to /home/jupyter/ds_cache/image_1400\n",
      "Writing to /home/jupyter/ds_cache/image_1500\n",
      "Writing to /home/jupyter/ds_cache/image_1600\n",
      "Writing to /home/jupyter/ds_cache/image_1700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0.6130, 0.4126, 0.6595]), tensor([0.1417, 0.2045, 0.1237]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_mean_std(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean: [0.65806392 0.4906465  0.69688281] , std: [0.15952521 0.24276932 0.13793028]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
